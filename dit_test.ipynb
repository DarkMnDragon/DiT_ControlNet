{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from IPython.display import clear_output\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from scipy.integrate import solve_ivp\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from dit_skip import DiT\n",
    "from dit_controlnet import DiTControlNet\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda:0\"\n",
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "DiT                                      [128, 3, 32, 32]          131,072\n",
      "├─PatchEmbed: 1-1                        [128, 256, 512]           --\n",
      "│    └─Conv2d: 2-1                       [128, 512, 16, 16]        6,656\n",
      "│    └─Identity: 2-2                     [128, 256, 512]           --\n",
      "├─TimestepEmbedder: 1-2                  [128, 512]                --\n",
      "│    └─Sequential: 2-3                   [128, 512]                --\n",
      "│    │    └─Linear: 3-1                  [128, 512]                131,584\n",
      "│    │    └─SiLU: 3-2                    [128, 512]                --\n",
      "│    │    └─Linear: 3-3                  [128, 512]                262,656\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─DiTBlock: 2-4                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-4              [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-5               [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-6               [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-7               [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-8                     [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-5                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-9              [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-10              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-11              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-12              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-13                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-6                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-14             [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-15              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-16              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-17              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-18                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-7                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-19             [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-20              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-21              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-22              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-23                    [128, 256, 512]           2,099,712\n",
      "├─DiTBlock: 1-4                          [128, 256, 512]           --\n",
      "│    └─Sequential: 2-8                   [128, 3072]               --\n",
      "│    │    └─SiLU: 3-24                   [128, 512]                --\n",
      "│    │    └─Linear: 3-25                 [128, 3072]               1,575,936\n",
      "│    └─LayerNorm: 2-9                    [128, 256, 512]           --\n",
      "│    └─Attention: 2-10                   [128, 256, 512]           --\n",
      "│    │    └─Linear: 3-26                 [128, 256, 1536]          787,968\n",
      "│    │    └─Identity: 3-27               [128, 8, 256, 64]         --\n",
      "│    │    └─Identity: 3-28               [128, 8, 256, 64]         --\n",
      "│    │    └─Linear: 3-29                 [128, 256, 512]           262,656\n",
      "│    │    └─Dropout: 3-30                [128, 256, 512]           --\n",
      "│    └─LayerNorm: 2-11                   [128, 256, 512]           --\n",
      "│    └─Mlp: 2-12                         [128, 256, 512]           --\n",
      "│    │    └─Linear: 3-31                 [128, 256, 2048]          1,050,624\n",
      "│    │    └─GELU: 3-32                   [128, 256, 2048]          --\n",
      "│    │    └─Dropout: 3-33                [128, 256, 2048]          --\n",
      "│    │    └─Identity: 3-34               [128, 256, 2048]          --\n",
      "│    │    └─Linear: 3-35                 [128, 256, 512]           1,049,088\n",
      "│    │    └─Dropout: 3-36                [128, 256, 512]           --\n",
      "├─ModuleList: 1-5                        --                        --\n",
      "│    └─DiTBlock: 2-13                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-37             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-38                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-39              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-40              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-41              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-42                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-14                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-43             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-44                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-45              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-46              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-47              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-48                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-15                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-49             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-50                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-51              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-52              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-53              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-54                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-16                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-55             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-56                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-57              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-58              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-59              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-60                    [128, 256, 512]           2,099,712\n",
      "├─FinalLayer: 1-6                        [128, 256, 12]            --\n",
      "│    └─Sequential: 2-17                  [128, 1024]               --\n",
      "│    │    └─SiLU: 3-61                   [128, 512]                --\n",
      "│    │    └─Linear: 3-62                 [128, 1024]               525,312\n",
      "│    └─LayerNorm: 2-18                   [128, 256, 512]           --\n",
      "│    └─Linear: 2-19                      [128, 256, 12]            6,156\n",
      "├─Conv2d: 1-7                            [128, 3, 32, 32]          84\n",
      "==========================================================================================\n",
      "Total params: 45,699,168\n",
      "Trainable params: 45,568,096\n",
      "Non-trainable params: 131,072\n",
      "Total mult-adds (G): 6.06\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 11579.42\n",
      "Params size (MB): 182.27\n",
      "Estimated Total Size (MB): 11763.27\n",
      "==========================================================================================\n",
      "torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.randn(128, 3, 32, 32).to(device)    # Input image tensor\n",
    "t = torch.rand((128,),).to(device)            # Noise labels (timestep embedding)\n",
    "\n",
    "dit = DiT(input_size=32,\n",
    "                 patch_size=2,\n",
    "                 in_channels=3,\n",
    "                 out_channels=3,\n",
    "\t\t\t\t hidden_size=512,\n",
    "\t\t\t\t depth=9,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=4,\n",
    "                 num_classes=0,\n",
    "                 use_long_skip=True,\n",
    "\t\t\t\t final_conv=True).to(device)\n",
    "\n",
    "print(summary(dit, input_data=[imgs, t]))\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = dit(imgs, t)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = \"/root/autodl-tmp/dit-1rf/003-DiT-S/2/checkpoints/0180000.pt\"\n",
    "# state_dict = torch.load(ckpt_path, map_location=lambda storage, loc: storage)[\"ema_ode\"]\n",
    "# model_ode_dict = dit.state_dict()\n",
    "\n",
    "# missing_keys, unexpected_keys = dit.load_state_dict(state_dict)\n",
    "# dit.eval()\n",
    "# print(model_ode_dict.keys())\n",
    "# print(missing_keys)\n",
    "# print(unexpected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): PatchEmbed(\n",
       "    (proj): Conv2d(3, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (in_blocks): ModuleList(\n",
       "    (0-3): 4 x DiTBlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): DiTBlock(\n",
       "    (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (act): GELU(approximate='tanh')\n",
       "      (drop1): Dropout(p=0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (drop2): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (out_blocks): ModuleList(\n",
       "    (0-3): 4 x DiTBlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "      )\n",
       "      (skip_linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=512, out_features=12, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlnet = DiTControlNet.from_transformer(transformer=dit,\n",
    "                                            input_size=32,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpatch_size=2,\n",
    "\t\t\t\t\t\t\t\t\t\t\tin_channels=3,\n",
    "\t\t\t\t\t\t\t\t\t\t\thidden_size=512,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdepth=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_heads=8,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmlp_ratio=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_classes=0,\n",
    "                                            load_weights_from_transformer=True).to(device)\n",
    "controlnet.train()\n",
    "\n",
    "dit.requires_grad_(False)\n",
    "dit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_cond = torch.randn_like(imgs)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tcontrolnet_block_samples, controlnet_mid_block_sample = controlnet(x=imgs,\n",
    "\t\t\t\t\tcontrolnet_x=controlnet_cond,\n",
    "\t\t\t\t\tt=t,\n",
    "\t\t\t\t\tconditioning_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n",
      "torch.Size([128, 256, 512])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(len(controlnet_block_samples))\n",
    "print(len(controlnet_mid_block_sample))\n",
    "\n",
    "print(controlnet_block_samples[0].shape)\n",
    "print(controlnet_block_samples[0].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add controlnet_block_samples[3] to decoder block 0\n",
      "Add controlnet_block_samples[2] to decoder block 1\n",
      "Add controlnet_block_samples[1] to decoder block 2\n",
      "Add controlnet_block_samples[0] to decoder block 3\n",
      "torch.Size([128, 3, 32, 32])\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\toutput_controlnet = dit(imgs, t, \n",
    "\t\t\t\tcontrolnet_block_samples=controlnet_block_samples,\n",
    "\t\t\t\tcontrolnet_mid_block_sample=controlnet_mid_block_sample)\n",
    "\n",
    "print(output_controlnet.shape)\n",
    "print(output_controlnet.requires_grad)\n",
    "\n",
    "torch.isclose(output, output_controlnet).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
