{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from IPython.display import clear_output\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from scipy.integrate import solve_ivp\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from dit_skip import DiT\n",
    "from controlnet_dit import DiTControlNet\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda:0\"\n",
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "DiT                                      [128, 3, 32, 32]          131,072\n",
      "├─PatchEmbed: 1-1                        [128, 256, 512]           --\n",
      "│    └─Conv2d: 2-1                       [128, 512, 16, 16]        6,656\n",
      "│    └─Identity: 2-2                     [128, 256, 512]           --\n",
      "├─TimestepEmbedder: 1-2                  [128, 512]                --\n",
      "│    └─Sequential: 2-3                   [128, 512]                --\n",
      "│    │    └─Linear: 3-1                  [128, 512]                131,584\n",
      "│    │    └─SiLU: 3-2                    [128, 512]                --\n",
      "│    │    └─Linear: 3-3                  [128, 512]                262,656\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─DiTBlock: 2-4                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-4              [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-5               [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-6               [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-7               [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-8                     [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-5                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-9              [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-10              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-11              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-12              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-13                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-6                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-14             [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-15              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-16              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-17              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-18                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-7                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-19             [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-20              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-21              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-22              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-23                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-8                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-24             [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-25              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-26              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-27              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-28                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-9                     [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-29             [128, 3072]               1,575,936\n",
      "│    │    └─LayerNorm: 3-30              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-31              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-32              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-33                    [128, 256, 512]           2,099,712\n",
      "├─DiTBlock: 1-4                          [128, 256, 512]           --\n",
      "│    └─Sequential: 2-10                  [128, 3072]               --\n",
      "│    │    └─SiLU: 3-34                   [128, 512]                --\n",
      "│    │    └─Linear: 3-35                 [128, 3072]               1,575,936\n",
      "│    └─LayerNorm: 2-11                   [128, 256, 512]           --\n",
      "│    └─Attention: 2-12                   [128, 256, 512]           --\n",
      "│    │    └─Linear: 3-36                 [128, 256, 1536]          787,968\n",
      "│    │    └─Identity: 3-37               [128, 8, 256, 64]         --\n",
      "│    │    └─Identity: 3-38               [128, 8, 256, 64]         --\n",
      "│    │    └─Linear: 3-39                 [128, 256, 512]           262,656\n",
      "│    │    └─Dropout: 3-40                [128, 256, 512]           --\n",
      "│    └─LayerNorm: 2-13                   [128, 256, 512]           --\n",
      "│    └─Mlp: 2-14                         [128, 256, 512]           --\n",
      "│    │    └─Linear: 3-41                 [128, 256, 2048]          1,050,624\n",
      "│    │    └─GELU: 3-42                   [128, 256, 2048]          --\n",
      "│    │    └─Dropout: 3-43                [128, 256, 2048]          --\n",
      "│    │    └─Identity: 3-44               [128, 256, 2048]          --\n",
      "│    │    └─Linear: 3-45                 [128, 256, 512]           1,049,088\n",
      "│    │    └─Dropout: 3-46                [128, 256, 512]           --\n",
      "├─ModuleList: 1-5                        --                        --\n",
      "│    └─DiTBlock: 2-15                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-47             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-48                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-49              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-50              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-51              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-52                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-16                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-53             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-54                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-55              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-56              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-57              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-58                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-17                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-59             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-60                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-61              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-62              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-63              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-64                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-18                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-65             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-66                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-67              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-68              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-69              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-70                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-19                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-71             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-72                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-73              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-74              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-75              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-76                    [128, 256, 512]           2,099,712\n",
      "│    └─DiTBlock: 2-20                    [128, 256, 512]           --\n",
      "│    │    └─Sequential: 3-77             [128, 3072]               1,575,936\n",
      "│    │    └─Linear: 3-78                 [128, 256, 512]           524,800\n",
      "│    │    └─LayerNorm: 3-79              [128, 256, 512]           --\n",
      "│    │    └─Attention: 3-80              [128, 256, 512]           1,050,624\n",
      "│    │    └─LayerNorm: 3-81              [128, 256, 512]           --\n",
      "│    │    └─Mlp: 3-82                    [128, 256, 512]           2,099,712\n",
      "├─FinalLayer: 1-6                        [128, 256, 12]            --\n",
      "│    └─Sequential: 2-21                  [128, 1024]               --\n",
      "│    │    └─SiLU: 3-83                   [128, 512]                --\n",
      "│    │    └─Linear: 3-84                 [128, 1024]               525,312\n",
      "│    └─LayerNorm: 2-22                   [128, 256, 512]           --\n",
      "│    └─Linear: 2-23                      [128, 256, 12]            6,156\n",
      "├─Conv2d: 1-7                            [128, 3, 32, 32]          84\n",
      "==========================================================================================\n",
      "Total params: 65,653,856\n",
      "Trainable params: 65,522,784\n",
      "Non-trainable params: 131,072\n",
      "Total mult-adds (G): 8.62\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 16692.28\n",
      "Params size (MB): 262.09\n",
      "Estimated Total Size (MB): 16955.95\n",
      "==========================================================================================\n",
      "torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.randn(128, 3, 32, 32).to(device)    # Input image tensor\n",
    "t = torch.rand((128,),).to(device)            # Noise labels (timestep embedding)\n",
    "\n",
    "dit = DiT(input_size=32,\n",
    "                 patch_size=2,\n",
    "                 in_channels=3,\n",
    "                 out_channels=3,\n",
    "\t\t\t\t hidden_size=512,\n",
    "\t\t\t\t depth=13,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=4,\n",
    "                 num_classes=0,\n",
    "                 use_long_skip=True,\n",
    "\t\t\t\t final_conv=True).to(device)\n",
    "\n",
    "print(summary(dit, input_data=[imgs, t]))\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = dit(imgs, t)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_420857/2936688020.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_path, map_location=lambda storage, loc: storage)[\"ema_ode\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['pos_embed', 'x_embedder.proj.weight', 'x_embedder.proj.bias', 't_embedder.mlp.0.weight', 't_embedder.mlp.0.bias', 't_embedder.mlp.2.weight', 't_embedder.mlp.2.bias', 'in_blocks.0.attn.qkv.weight', 'in_blocks.0.attn.qkv.bias', 'in_blocks.0.attn.proj.weight', 'in_blocks.0.attn.proj.bias', 'in_blocks.0.mlp.fc1.weight', 'in_blocks.0.mlp.fc1.bias', 'in_blocks.0.mlp.fc2.weight', 'in_blocks.0.mlp.fc2.bias', 'in_blocks.0.adaLN_modulation.1.weight', 'in_blocks.0.adaLN_modulation.1.bias', 'in_blocks.1.attn.qkv.weight', 'in_blocks.1.attn.qkv.bias', 'in_blocks.1.attn.proj.weight', 'in_blocks.1.attn.proj.bias', 'in_blocks.1.mlp.fc1.weight', 'in_blocks.1.mlp.fc1.bias', 'in_blocks.1.mlp.fc2.weight', 'in_blocks.1.mlp.fc2.bias', 'in_blocks.1.adaLN_modulation.1.weight', 'in_blocks.1.adaLN_modulation.1.bias', 'in_blocks.2.attn.qkv.weight', 'in_blocks.2.attn.qkv.bias', 'in_blocks.2.attn.proj.weight', 'in_blocks.2.attn.proj.bias', 'in_blocks.2.mlp.fc1.weight', 'in_blocks.2.mlp.fc1.bias', 'in_blocks.2.mlp.fc2.weight', 'in_blocks.2.mlp.fc2.bias', 'in_blocks.2.adaLN_modulation.1.weight', 'in_blocks.2.adaLN_modulation.1.bias', 'in_blocks.3.attn.qkv.weight', 'in_blocks.3.attn.qkv.bias', 'in_blocks.3.attn.proj.weight', 'in_blocks.3.attn.proj.bias', 'in_blocks.3.mlp.fc1.weight', 'in_blocks.3.mlp.fc1.bias', 'in_blocks.3.mlp.fc2.weight', 'in_blocks.3.mlp.fc2.bias', 'in_blocks.3.adaLN_modulation.1.weight', 'in_blocks.3.adaLN_modulation.1.bias', 'in_blocks.4.attn.qkv.weight', 'in_blocks.4.attn.qkv.bias', 'in_blocks.4.attn.proj.weight', 'in_blocks.4.attn.proj.bias', 'in_blocks.4.mlp.fc1.weight', 'in_blocks.4.mlp.fc1.bias', 'in_blocks.4.mlp.fc2.weight', 'in_blocks.4.mlp.fc2.bias', 'in_blocks.4.adaLN_modulation.1.weight', 'in_blocks.4.adaLN_modulation.1.bias', 'in_blocks.5.attn.qkv.weight', 'in_blocks.5.attn.qkv.bias', 'in_blocks.5.attn.proj.weight', 'in_blocks.5.attn.proj.bias', 'in_blocks.5.mlp.fc1.weight', 'in_blocks.5.mlp.fc1.bias', 'in_blocks.5.mlp.fc2.weight', 'in_blocks.5.mlp.fc2.bias', 'in_blocks.5.adaLN_modulation.1.weight', 'in_blocks.5.adaLN_modulation.1.bias', 'mid_block.attn.qkv.weight', 'mid_block.attn.qkv.bias', 'mid_block.attn.proj.weight', 'mid_block.attn.proj.bias', 'mid_block.mlp.fc1.weight', 'mid_block.mlp.fc1.bias', 'mid_block.mlp.fc2.weight', 'mid_block.mlp.fc2.bias', 'mid_block.adaLN_modulation.1.weight', 'mid_block.adaLN_modulation.1.bias', 'out_blocks.0.attn.qkv.weight', 'out_blocks.0.attn.qkv.bias', 'out_blocks.0.attn.proj.weight', 'out_blocks.0.attn.proj.bias', 'out_blocks.0.mlp.fc1.weight', 'out_blocks.0.mlp.fc1.bias', 'out_blocks.0.mlp.fc2.weight', 'out_blocks.0.mlp.fc2.bias', 'out_blocks.0.adaLN_modulation.1.weight', 'out_blocks.0.adaLN_modulation.1.bias', 'out_blocks.0.skip_linear.weight', 'out_blocks.0.skip_linear.bias', 'out_blocks.1.attn.qkv.weight', 'out_blocks.1.attn.qkv.bias', 'out_blocks.1.attn.proj.weight', 'out_blocks.1.attn.proj.bias', 'out_blocks.1.mlp.fc1.weight', 'out_blocks.1.mlp.fc1.bias', 'out_blocks.1.mlp.fc2.weight', 'out_blocks.1.mlp.fc2.bias', 'out_blocks.1.adaLN_modulation.1.weight', 'out_blocks.1.adaLN_modulation.1.bias', 'out_blocks.1.skip_linear.weight', 'out_blocks.1.skip_linear.bias', 'out_blocks.2.attn.qkv.weight', 'out_blocks.2.attn.qkv.bias', 'out_blocks.2.attn.proj.weight', 'out_blocks.2.attn.proj.bias', 'out_blocks.2.mlp.fc1.weight', 'out_blocks.2.mlp.fc1.bias', 'out_blocks.2.mlp.fc2.weight', 'out_blocks.2.mlp.fc2.bias', 'out_blocks.2.adaLN_modulation.1.weight', 'out_blocks.2.adaLN_modulation.1.bias', 'out_blocks.2.skip_linear.weight', 'out_blocks.2.skip_linear.bias', 'out_blocks.3.attn.qkv.weight', 'out_blocks.3.attn.qkv.bias', 'out_blocks.3.attn.proj.weight', 'out_blocks.3.attn.proj.bias', 'out_blocks.3.mlp.fc1.weight', 'out_blocks.3.mlp.fc1.bias', 'out_blocks.3.mlp.fc2.weight', 'out_blocks.3.mlp.fc2.bias', 'out_blocks.3.adaLN_modulation.1.weight', 'out_blocks.3.adaLN_modulation.1.bias', 'out_blocks.3.skip_linear.weight', 'out_blocks.3.skip_linear.bias', 'out_blocks.4.attn.qkv.weight', 'out_blocks.4.attn.qkv.bias', 'out_blocks.4.attn.proj.weight', 'out_blocks.4.attn.proj.bias', 'out_blocks.4.mlp.fc1.weight', 'out_blocks.4.mlp.fc1.bias', 'out_blocks.4.mlp.fc2.weight', 'out_blocks.4.mlp.fc2.bias', 'out_blocks.4.adaLN_modulation.1.weight', 'out_blocks.4.adaLN_modulation.1.bias', 'out_blocks.4.skip_linear.weight', 'out_blocks.4.skip_linear.bias', 'out_blocks.5.attn.qkv.weight', 'out_blocks.5.attn.qkv.bias', 'out_blocks.5.attn.proj.weight', 'out_blocks.5.attn.proj.bias', 'out_blocks.5.mlp.fc1.weight', 'out_blocks.5.mlp.fc1.bias', 'out_blocks.5.mlp.fc2.weight', 'out_blocks.5.mlp.fc2.bias', 'out_blocks.5.adaLN_modulation.1.weight', 'out_blocks.5.adaLN_modulation.1.bias', 'out_blocks.5.skip_linear.weight', 'out_blocks.5.skip_linear.bias', 'final_layer.linear.weight', 'final_layer.linear.bias', 'final_layer.adaLN_modulation.1.weight', 'final_layer.adaLN_modulation.1.bias', 'final_conv.weight', 'final_conv.bias'])\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"/root/autodl-tmp/dit-1rf/003-DiT-S/2/checkpoints/0180000.pt\"\n",
    "state_dict = torch.load(ckpt_path, map_location=lambda storage, loc: storage)[\"ema_ode\"]\n",
    "model_ode_dict = dit.state_dict()\n",
    "\n",
    "missing_keys, unexpected_keys = dit.load_state_dict(state_dict)\n",
    "dit.eval()\n",
    "print(model_ode_dict.keys())\n",
    "print(missing_keys)\n",
    "print(unexpected_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected keys in controlnet.in_blocks: ['3.attn.qkv.weight', '3.attn.qkv.bias', '3.attn.proj.weight', '3.attn.proj.bias', '3.mlp.fc1.weight', '3.mlp.fc1.bias', '3.mlp.fc2.weight', '3.mlp.fc2.bias', '3.adaLN_modulation.1.weight', '3.adaLN_modulation.1.bias', '4.attn.qkv.weight', '4.attn.qkv.bias', '4.attn.proj.weight', '4.attn.proj.bias', '4.mlp.fc1.weight', '4.mlp.fc1.bias', '4.mlp.fc2.weight', '4.mlp.fc2.bias', '4.adaLN_modulation.1.weight', '4.adaLN_modulation.1.bias', '5.attn.qkv.weight', '5.attn.qkv.bias', '5.attn.proj.weight', '5.attn.proj.bias', '5.mlp.fc1.weight', '5.mlp.fc1.bias', '5.mlp.fc2.weight', '5.mlp.fc2.bias', '5.adaLN_modulation.1.weight', '5.adaLN_modulation.1.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): PatchEmbed(\n",
       "    (proj): Conv2d(3, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (in_blocks): ModuleList(\n",
       "    (0-5): 6 x DiTBlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): DiTBlock(\n",
       "    (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "      (q_norm): Identity()\n",
       "      (k_norm): Identity()\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (act): GELU(approximate='tanh')\n",
       "      (drop1): Dropout(p=0, inplace=False)\n",
       "      (norm): Identity()\n",
       "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (drop2): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (out_blocks): ModuleList(\n",
       "    (0-5): 6 x DiTBlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=3072, bias=True)\n",
       "      )\n",
       "      (skip_linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((512,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=512, out_features=12, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlnet = DiTControlNet.from_transformer(transformer=dit,\n",
    "                                            input_size=32,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpatch_size=2,\n",
    "\t\t\t\t\t\t\t\t\t\t\tin_channels=3,\n",
    "\t\t\t\t\t\t\t\t\t\t\thidden_size=512,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdepth=3,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_heads=8,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmlp_ratio=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_classes=0,\n",
    "                                            load_weights_from_transformer=True).to(device)\n",
    "controlnet.train()\n",
    "\n",
    "dit.requires_grad_(False)\n",
    "dit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_cond = torch.randn_like(imgs)\n",
    "\n",
    "controlnet_block_samples, controlnet_mid_block_sample = controlnet(x=imgs,\n",
    "\t\t\t\t\tcontrolnet_x=controlnet_cond,\n",
    "\t\t\t\t\tt=t,\n",
    "\t\t\t\t\tconditioning_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(controlnet_block_samples))\n",
    "print(len(controlnet_mid_block_sample))\n",
    "\n",
    "print(controlnet_block_samples[0].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\toutput_controlnet = dit(imgs, t, \n",
    "\t\t\t\tcontrolnet_block_samples=controlnet_block_samples,\n",
    "\t\t\t\tcontrolnet_mid_block_sample=controlnet_mid_block_sample)\n",
    "\n",
    "print(output_controlnet.shape)\n",
    "print(output_controlnet.requires_grad)\n",
    "\n",
    "torch.isclose(output, output_controlnet).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
